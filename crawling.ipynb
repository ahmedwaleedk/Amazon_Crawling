{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wv0gJfMO9Xqk",
        "outputId": "e5e690f4-d664-47f4-9d97-df4932815708",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'streamlit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8d0f49f8832c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstreamlit\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maltair\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfpdf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFPDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "from fpdf import FPDF\n",
        "from io import BytesIO\n",
        "from crawler import crawl_website\n",
        "\n",
        "st.set_page_config(page_title=\"IR Web Crawler\", layout=\"wide\")\n",
        "tab1, tab2, tab3 = st.tabs([\"üìå Project Overview\", \"üîç Analysis Results\", \"üì§ Download Report\"])\n",
        "\n",
        "# Store crawl results\n",
        "results = []\n",
        "\n",
        "with tab1:\n",
        "    st.title(\"IR Project: Intelligent Web Crawler & Analyzer\")\n",
        "    st.write(\"üë§ Omar Ehab 232125 - UI & Report Designer\")\n",
        "    st.markdown(\"### Description\")\n",
        "    st.write(\n",
        "        \"This app allows you to crawl a website, analyze how many links each page contains, \"\n",
        "        \"visualize the data, and download the results as CSV or PDF.\"\n",
        "    )\n",
        "\n",
        "with tab2:\n",
        "    st.subheader(\"Analysis Results\")\n",
        "    start_url = st.text_input(\"Enter a URL to crawl\", value=\"https://example.com\")\n",
        "    max_pages = st.slider(\"Max pages to crawl\", 5, 50, 10)\n",
        "\n",
        "    if st.button(\"Start Crawling\"):\n",
        "        with st.spinner(\"Crawling in progress...\"):\n",
        "            results = crawl_website(start_url, max_pages=max_pages)\n",
        "        st.success(f\"‚úÖ Crawled {len(results)} pages.\")\n",
        "\n",
        "        # Filter valid results for visualization and export\n",
        "        clean_results = [r for r in results if \"error\" not in r]\n",
        "        df = pd.DataFrame(clean_results)\n",
        "\n",
        "        for res in results:\n",
        "            if \"error\" in res:\n",
        "                st.error(f\"{res['url']} - ‚ùå {res['error']}\")\n",
        "            else:\n",
        "                st.write(f\"üîó [{res['title']}]({res['url']}) - {res['num_links']} links found\")\n",
        "\n",
        "        # Bar Chart\n",
        "        if not df.empty:\n",
        "            st.markdown(\"### üìä Link Distribution\")\n",
        "            chart = (\n",
        "                alt.Chart(df)\n",
        "                .mark_bar()\n",
        "                .encode(\n",
        "                    x=alt.X(\"title\", sort=\"-y\", title=\"Page Title\"),\n",
        "                    y=alt.Y(\"num_links\", title=\"Number of Links\"),\n",
        "                    tooltip=[\"title\", \"num_links\", \"url\"]\n",
        "                )\n",
        "                .properties(width=700)\n",
        "            )\n",
        "            st.altair_chart(chart, use_container_width=True)\n",
        "\n",
        "with tab3:\n",
        "    st.subheader(\"üì• Export Results\")\n",
        "\n",
        "    if results:\n",
        "        clean_results = [r for r in results if \"error\" not in r]\n",
        "        df = pd.DataFrame(clean_results)\n",
        "\n",
        "        # Download as CSV\n",
        "        csv = df.to_csv(index=False).encode(\"utf-8\")\n",
        "        st.download_button(\"‚¨á Download CSV\", data=csv, file_name=\"crawl_results.csv\", mime=\"text/csv\")\n",
        "\n",
        "        # Generate PDF\n",
        "        pdf = FPDF()\n",
        "        pdf.add_page()\n",
        "        pdf.set_font(\"Arial\", \"B\", 16)\n",
        "        pdf.cell(0, 10, \"Web Crawl Report\", ln=True, align=\"C\")\n",
        "        pdf.ln(10)\n",
        "\n",
        "        pdf.set_font(\"Arial\", size=12)\n",
        "        for r in clean_results:\n",
        "            pdf.multi_cell(0, 10, f\"{r['title']} ({r['url']}) - {r['num_links']} links\", border=0)\n",
        "            pdf.ln(1)\n",
        "\n",
        "        # Export PDF to bytes\n",
        "        pdf_bytes = pdf.output(dest='S').encode('latin1')\n",
        "        st.download_button(\n",
        "            \"üßæ Download PDF\",\n",
        "            data=pdf_bytes,\n",
        "            file_name=\"crawl_report.pdf\",\n",
        "            mime=\"application/pdf\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        st.info(\"üîç Run a crawl first to enable¬†downloads.\")"
      ]
    }
  ]
}