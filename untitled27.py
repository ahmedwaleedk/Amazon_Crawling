# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-G9dNC-pTv3eTYNggCBQWTFuOHdViS_R
"""

#Member 1
import requests

robots_url = "https://www.amazon.com/robots.txt"
response = requests.get(robots_url)
print(response.text)

#Member 1
import urllib.robotparser

# Load robots.txt from Amazon
rp = urllib.robotparser.RobotFileParser()
rp.set_url("https://www.amazon.com/robots.txt")
rp.read()

# Example URL tests
test_urls = [
    "https://www.amazon.com/gp/product/B09G3HRMVB",       # Product page
    "https://www.amazon.com/gp/cart",                     # Cart (blocked)
    "https://www.amazon.com/s?k=laptops",                 # Search results
    "https://www.amazon.com/gp/help/customer/contact-us", # Help page
]

user_agent = "*"

for url in test_urls:
    allowed = rp.can_fetch(user_agent, url)
    print(f"{'‚úÖ ALLOWED' if allowed else '‚ùå DISALLOWED'}: {url}")

# Step 1: Install dependencies
!apt-get update
!apt-get install -y chromium-browser # Install chromium-browser
!pip install selenium pandas webdriver-manager # Install selenium, pandas, and webdriver-manager

!pip install selenium fake-useragent webdriver-manager > /dev/null
!apt-get update > /dev/null
!apt install chromium-chromedriver > /dev/null

# ---------- IMPORT LIBRARIES ----------
import os
import time
import csv
import requests
import pandas as pd
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from fake_useragent import UserAgent
from webdriver_manager.chrome import ChromeDriverManager
from google.colab import files

# ---------- CONFIGURATION ----------
SCRAPER_API_KEY = "0ea79f2cc5a33a5fa5c555f9cb82e331"
keyword = "laptops"
MAX_PAGES = 5
HEADLESS = True

headers = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/114.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "en-US,en;q=0.9"
}

# ---------- FUNCTION: Chrome Options ----------
def get_user_agent():
    try:
        return UserAgent().random
    except:
        return headers["User-Agent"]

def get_chrome_options():
    options = Options()
    if HEADLESS:
        options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument(f'user-agent={get_user_agent()}')
    return options

# ---------- FUNCTION: Scrape Product Details ----------
def scrape_amazon_product(url):
    options = get_chrome_options()
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.get(url)
    time.sleep(3)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    driver.quit()

    name = soup.find(id='productTitle')
    name = name.get_text(strip=True) if name else 'N/A'

    price = soup.find('span', class_='a-offscreen')
    price = price.get_text(strip=True) if price else 'N/A'

    desc = soup.find('div', id='feature-bullets')
    if desc:
        description = ' '.join(b.get_text(strip=True) for b in desc.find_all('span') if b.get_text(strip=True))
    else:
        description = 'N/A'

    return {
        "name": name,
        "price": price,
        "description": description,
        "url": url
    }

# ---------- FUNCTION: Scrape Search Pages ----------
def get_search_result_links(keyword, max_pages=3):
    links = []
    for page in range(1, max_pages + 1):
        amazon_url = f"https://www.amazon.com/s?k={keyword}&page={page}"
        api_url = (
            f"http://api.scraperapi.com?api_key={SCRAPER_API_KEY}&url={amazon_url}&render=true"
        )

        print(f"\nüåê Visiting: {amazon_url}")
        try:
            response = requests.get(api_url, headers=headers)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Error fetching page {page}: {e}")
            continue

        soup = BeautifulSoup(response.content, "html.parser")
        results = soup.select("div.s-result-item[data-component-type='s-search-result']")
        print(f"üì¶ Page {page}: {len(results)} results found")

        for product in results:
            link_elem = product.select_one("a.a-link-normal.s-no-outline")
            if link_elem:
                href = link_elem["href"]
                link = "https://www.amazon.com" + href if href.startswith("/") else href
                links.append(link)

        time.sleep(2)  # polite scraping
    return links

# ---------- MAIN ----------
def main():
    print(f"üîé Searching for '{keyword}'...")
    product_links = get_search_result_links(keyword, MAX_PAGES)
    print(f"\nüîó Total links found: {len(product_links)}")

    all_data = []
    for url in product_links:
        print(f"‚û°Ô∏è Scraping product: {url}")
        try:
            data = scrape_amazon_product(url)
            all_data.append(data)
        except Exception as e:
            print(f"‚ùå Error scraping {url}: {e}")

    # Save to CSV
    df = pd.DataFrame(all_data)
    filename = f"amazon_{keyword}_products_full.csv"
    df.to_csv(filename, index=False)
    print(f"\n‚úÖ Scraping complete! Saved {len(df)} products to '{filename}'")
    #print(df.head())

    files.download(filename)

# ---------- RUN ----------
main()