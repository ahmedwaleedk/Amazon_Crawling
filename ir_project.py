# -*- coding: utf-8 -*-
"""IR project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14nCZQdi7phcH18i3dwNpUWonDmz4fDVJ
"""

#Member 1
import requests

robots_url = "https://www.amazon.com/robots.txt"
response = requests.get(robots_url)
print(response.text)

#Member 1
import urllib.robotparser

# Load robots.txt from Amazon
rp = urllib.robotparser.RobotFileParser()
rp.set_url("https://www.amazon.com/robots.txt")
rp.read()

# Example URL tests
test_urls = [
    "https://www.amazon.com/gp/product/B09G3HRMVB",       # Product page
    "https://www.amazon.com/gp/cart",                     # Cart (blocked)
    "https://www.amazon.com/s?k=laptops",                 # Search results
    "https://www.amazon.com/gp/help/customer/contact-us", # Help page
]

user_agent = "*"

for url in test_urls:
    allowed = rp.can_fetch(user_agent, url)
    print(f"{'‚úÖ ALLOWED' if allowed else '‚ùå DISALLOWED'}: {url}")

# Step 1: Install dependencies
!apt-get update
!apt-get install -y chromium-browser # Install chromium-browser
!pip install selenium pandas webdriver-manager # Install selenium, pandas, and webdriver-manager

!pip install selenium fake-useragent webdriver-manager > /dev/null
!apt-get update > /dev/null
!apt install chromium-chromedriver > /dev/null

# ---------- IMPORT LIBRARIES ----------
import os
import time
import csv
import requests
import pandas as pd
from bs4 import BeautifulSoup

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from fake_useragent import UserAgent
from webdriver_manager.chrome import ChromeDriverManager
from google.colab import files

# ---------- CONFIGURATION ----------
SCRAPER_API_KEY = "0ea79f2cc5a33a5fa5c555f9cb82e331"
keyword = "laptops"
MAX_PAGES = 5
HEADLESS = True

headers = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/114.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "en-US,en;q=0.9"
}

# ---------- FUNCTION: Chrome Options ----------
def get_user_agent():
    try:
        return UserAgent().random
    except:
        return headers["User-Agent"]

def get_chrome_options():
    options = Options()
    if HEADLESS:
        options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    options.add_argument(f'user-agent={get_user_agent()}')
    return options

# ---------- FUNCTION: Scrape Product Details ----------
def scrape_amazon_product(url):
    options = get_chrome_options()
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    driver.get(url)
    time.sleep(3)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    driver.quit()

    name = soup.find(id='productTitle')
    name = name.get_text(strip=True) if name else 'N/A'

    price = soup.find('span', class_='a-offscreen')
    price = price.get_text(strip=True) if price else 'N/A'

    desc = soup.find('div', id='feature-bullets')
    if desc:
        description = ' '.join(b.get_text(strip=True) for b in desc.find_all('span') if b.get_text(strip=True))
    else:
        description = 'N/A'

    return {
        "name": name,
        "price": price,
        "description": description,
        "url": url
    }

# ---------- FUNCTION: Scrape Search Pages ----------
def get_search_result_links(keyword, max_pages=3):
    links = []
    for page in range(1, max_pages + 1):
        amazon_url = f"https://www.amazon.com/s?k={keyword}&page={page}"
        api_url = (
            f"http://api.scraperapi.com?api_key={SCRAPER_API_KEY}&url={amazon_url}&render=true"
        )

        print(f"\nüåê Visiting: {amazon_url}")
        try:
            response = requests.get(api_url, headers=headers)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(f"‚ùå Error fetching page {page}: {e}")
            continue

        soup = BeautifulSoup(response.content, "html.parser")
        results = soup.select("div.s-result-item[data-component-type='s-search-result']")
        print(f"üì¶ Page {page}: {len(results)} results found")

        for product in results:
            link_elem = product.select_one("a.a-link-normal.s-no-outline")
            if link_elem:
                href = link_elem["href"]
                link = "https://www.amazon.com" + href if href.startswith("/") else href
                links.append(link)

        time.sleep(2)  # polite scraping
    return links

# ---------- MAIN ----------
def main():
    print(f"üîé Searching for '{keyword}'...")
    product_links = get_search_result_links(keyword, MAX_PAGES)
    print(f"\nüîó Total links found: {len(product_links)}")

    all_data = []
    for url in product_links:
        print(f"‚û°Ô∏è Scraping product: {url}")
        try:
            data = scrape_amazon_product(url)
            all_data.append(data)
        except Exception as e:
            print(f"‚ùå Error scraping {url}: {e}")

    # Save to CSV
    df = pd.DataFrame(all_data)
    filename = f"amazon_{keyword}_products_full.csv"
    df.to_csv(filename, index=False)
    print(f"\n‚úÖ Scraping complete! Saved {len(df)} products to '{filename}'")
    #print(df.head())

    files.download(filename)

# ---------- RUN ----------
main()

pip install selenium

from bs4 import BeautifulSoup
import requests

url = "https://www.amazon.com"

def is_js_heavy_site(url):
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, "html.parser")
        scripts = soup.find_all("script")
        if len(scripts) > 10 or any("application/ld+json" in str(script) for script in scripts):
            return True
        return False
    except Exception as e:
        print("Error checking site:", e)
        return None

print("JS-heavy site?" , is_js_heavy_site(url))

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re
import tempfile
import time
import requests

url = "https://www.amazon.com"

def find_rss_feeds(html, base_url):
    soup = BeautifulSoup(html, "html.parser")
    feeds = []
    for link in soup.find_all("link", type=["application/rss+xml", "application/atom+xml"]):
        href = link.get("href")
        if href:
            feeds.append(urljoin(base_url, href))
    return feeds

def find_possible_api_urls(html, base_url):
    soup = BeautifulSoup(html, "html.parser")
    api_urls = set()

    # Search for API-related href/src attributes
    for tag in soup.find_all(["a", "script", "link", "iframe", "source", "img"], href=True):
        href = tag.get("href")
        if href and ("/api/" in href.lower() or href.lower().endswith(".json")):
            api_urls.add(urljoin(base_url, href))

    for tag in soup.find_all(["a", "script", "link", "iframe", "source", "img"], src=True):
        src = tag.get("src")
        if src and ("/api/" in src.lower() or src.lower().endswith(".json")):
            api_urls.add(urljoin(base_url, src))

    # Search raw HTML for API-like patterns
    raw_api_urls = re.findall(r'https?://[^\s"\'>]+(?:/api/|\.json)[^\s"\'>]*', html, re.IGNORECASE)
    api_urls.update(raw_api_urls)

    return list(api_urls)

def run_selenium_and_extract(url):
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    # Create temporary user data directory
    user_data_dir = tempfile.mkdtemp()
    options.add_argument(f"--user-data-dir={user_data_dir}")

    try:
        driver = webdriver.Chrome(options=options)
        driver.get(url)
        time.sleep(5)  # Wait for JS to load

        html = driver.page_source
        print("‚úÖ Page loaded successfully using Selenium")

        rss = find_rss_feeds(html, url)
        apis = find_possible_api_urls(html, url)

        print("\nüì° RSS Feeds found (if any):")
        print(rss if rss else "No obvious RSS feeds found.")

        print("\nüîó Potential API Endpoints:")
        print(apis if apis else "No visible API endpoints found.")

        driver.quit()
    except Exception as e:
        print("‚ùå Selenium error:", e)

# Execute the task
run_selenium_and_extract(url)

from urllib.parse import urljoin

def check_robots_txt_for_feeds(url):
    try:
        robots_url = urljoin(url, "/robots.txt")
        resp = requests.get(robots_url)
        if resp.ok:
            lines = resp.text.splitlines()
            for line in lines:
                if "rss" in line.lower() or "api" in line.lower():
                    print("Mention in robots.txt:", line)
        else:
            print("robots.txt not accessible.")
    except Exception as e:
        print("Error reading robots.txt:", e)

check_robots_txt_for_feeds("https://www.amazon.com")

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import tempfile

options = Options()
options.add_argument("--headless")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

user_data_dir = tempfile.mkdtemp()
options.add_argument(f"--user-data-dir={user_data_dir}")

try:
    driver = webdriver.Chrome(options=options)
    driver.get(url)
    print("Page title via Selenium:", driver.title)
    driver.quit()
except Exception as e:
    print("Selenium error:", e)

!pip install streamlit pyngrok --quiet

from google.colab import files
uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import graphviz
# 
# # Load data from CSV
# @st.cache_data
# def load_data():
#     return pd.read_csv("amazon_laptops_products_full.csv")
# 
# data = load_data()
# 
# # Title
# st.title("üß† Intelligent Web Crawler & Analyzer")
# st.markdown("**Target Website:** Amazon (Search: Laptops)")
# 
# # Section 1 - Crawlability
# st.header("üîç Crawlability Overview")
# st.metric("Crawlability Score", "70 / 100")
# st.markdown("""
# - ‚úÖ Product/search pages allowed
# - ‚ùå Cart & account pages disallowed
# - ‚ùå Sitemap not found
# - ‚úÖ No crawl-delay detected
# """)
# 
# # Section 2 - Extracted Data Table
# st.header("üì¶ Top Extracted Product Data")
# st.dataframe(data.head(10))
# 
# # Section 3 - JS/API/RSS Detection
# st.header("‚öôÔ∏è Technology & Access Check")
# st.subheader("JavaScript-Heavy Site?")
# st.success("‚úÖ Yes ‚Äì JavaScript detected via multiple <script> tags")
# 
# st.subheader("RSS Feeds?")
# st.error("‚ùå No RSS feeds found")
# 
# st.subheader("API Endpoints?")
# st.info("üîó Few API-like links found in HTML (e.g., `/api/` or `.json`)")
# 
# # Section 4 - Crawling Recommendations
# st.header("üìå Crawling Recommendations")
# st.markdown("""
# - Use **Selenium** or **Playwright** for dynamic pages
# - No public API or RSS ‚Äî use **ScraperAPI** for rendering
# - Add retry delays & respect robots.txt
# """)
# 
# # Section 5 - Visual Sitemap
# st.header("üó∫Ô∏è Visual Sitemap")
# dot = graphviz.Digraph()
# dot.node("Home")
# dot.node("Search Results")
# dot.node("Product Page")
# dot.node("Cart (Blocked)")
# dot.edges([("Home", "Search Results"), ("Search Results", "Product Page")])
# st.graphviz_chart(dot)
#

from pyngrok import ngrok

# Set authtoken (only once per session)
ngrok.set_auth_token("2xNC5z81FSW84NRDlVNtJyChVbk_3UxQ63bTqPtZwamdYe8em")

from pyngrok import ngrok

# Launch ngrok tunnel to localhost:8501
public_url = ngrok.connect("http://localhost:8501")
print("üåê Streamlit URL:", public_url)

import streamlit as st
import pandas as pd

st.set_page_config(page_title="Amazon Crawler Dashboard", layout="wide")

st.title("üß† Intelligent Web Crawler & Analyzer")
st.markdown("### Website: [Amazon](https://www.amazon.com)")

# Section 1: Crawlability Summary
st.header("üìã Crawlability Summary")

crawlability = {
    "Allowed": [
        "/gp/product/B09G3HRMVB",
        "/s?k=laptops"
    ],
    "Disallowed": [
        "/gp/cart",
        "/gp/help/customer/contact-us"
    ]
}

col1, col2 = st.columns(2)
col1.metric("‚úÖ Allowed Paths", len(crawlability["Allowed"]))
col2.metric("‚ùå Disallowed Paths", len(crawlability["Disallowed"]))

with st.expander("See Crawlability Rules"):
    st.json(crawlability)

# Section 2: Extracted Data
st.header("üì¶ Top Extracted Products")

try:
    df = pd.read_csv("amazon_laptops_products_full.csv")
    st.dataframe(df.head(10), use_container_width=True)
except:
    st.warning("Product CSV not found. Please run the scraper first.")

# Section 3: JS, RSS, and API Info
st.header("üï∏Ô∏è Site Behavior Analysis")

js_heavy = True
rss_found = []
api_found = ["https://www.amazon.com/api/example.json"]

st.markdown(f"**JavaScript-heavy site:** {'‚úÖ Yes' if js_heavy else '‚ùå No'}")
st.markdown("**RSS Feeds Found:** " + (', '.join(rss_found) if rss_found else "None"))
st.markdown("**API Endpoints Found:**")
st.code("\n".join(api_found) if api_found else "No public APIs detected.")

# Section 4: Tool Recommendations
st.header("üõ† Recommended Tools")

st.markdown("""
- `requests` + `BeautifulSoup` (basic HTML pages)
- `Selenium` or `Playwright` (JavaScript-rendered content)
- `ScraperAPI` or `Rotating Proxies` for large-scale scraping
- `pandas` for data storage & cleaning
""")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# 
# st.title("üöÄ Hello from Streamlit in Colab!")
# st.write("This Streamlit app is running via ngrok.")
#

!pip install streamlit pyngrok

from pyngrok import conf

conf.get_default().auth_token = "2xNC5z81FSW84NRDlVNtJyChVbk_3UxQ63bTqPtZwamdYe8em"

!streamlit run app.py &

from pyngrok import ngrok

public_url = ngrok.connect(8501)
print("üåê Public Streamlit URL:", public_url)

!streamlit run app.py &>/content/logs.txt &

!ngrok config add-authtoken 2xNJZIxkc99D6Mis5rWHwGD9CPG_62Gf5EbNBfTaUiMHSiYgQ

from pyngrok import ngrok
ngrok.kill()

public_url = ngrok.connect("8501", "http")
print("üåê Public URL:", public_url)

st.set_page_config(page_title="Web Crawler & Analyzer", layout="wide")
st.title("üåê Intelligent Web Crawler & Analyzer")
st.markdown("### Member 4: Visual & Report Designer")

import streamlit as st
import pandas as pd

# üß™ Temporary fake data to test your layout
crawled_data_df = pd.DataFrame({
    "URL": ["https://example.com", "https://openai.com"],
    "Title": ["Example Domain", "OpenAI"],
    "Keywords": [["example", "domain"], ["AI", "machine learning"]],
})

# Display with title
st.markdown("## üîç Analysis Results (Mock Data)")
st.dataframe(crawled_data_df)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# 
# # Title
# st.markdown("# üìä Intelligent Web Crawler - Member 4 View")
# 
# # Tabs for layout
# tab1, tab2, tab3 = st.tabs(["Overview", "Analysis Results", "Report"])
# 
# with tab1:
#     st.write("Welcome! This is the frontend designed by Member 4.")
#     st.write("It displays the crawled and analyzed data below.")
# 
# with tab2:
#     # Mock data (replace this with real output later)
#     crawled_data_df = pd.DataFrame({
#         "URL": ["https://example.com", "https://openai.com"],
#         "Title": ["Example Domain", "OpenAI"],
#         "Keywords": [["example", "domain"], ["AI", "machine learning"]],
#     })
#     st.markdown("### üîç Analysis Results")
#     st.dataframe(crawled_data_df)
# 
# with tab3:
#     st.markdown("### üì• Download Report")
#     st.download_button(
#         label="Download mock CSV",
#         data=crawled_data_df.to_csv(index=False).encode('utf-8'),
#         file_name='crawled_data.csv',
#         mime='text/csv'
#     )
#

!streamlit run app.py &

from pyngrok import ngrok

# If ngrok was already running, kill the previous tunnel first
ngrok.kill()  # To avoid the "only 1 tunnel" error

# Connect to the Streamlit app
public_url = ngrok.connect(8501)
print("üåê Public URL:", public_url)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# 
# st.set_page_config(page_title="IR Project", layout="wide")
# 
# st.markdown("# üöÄ Intelligent Web Crawler (Member 4 UI)")
# 
# tab1, tab2, tab3 = st.tabs(["Overview", "Analysis Results", "Report"])
# 
# with tab1:
#     st.write("Frontend by Member 4. Displays and downloads results.")
# 
# with tab2:
#     crawled_data_df = pd.DataFrame({
#         "URL": ["https://example.com", "https://openai.com"],
#         "Title": ["Example Domain", "OpenAI"],
#         "Keywords": [["example", "domain"], ["AI", "machine learning"]],
#     })
#     st.write("### üîç Analysis Results")
#     st.dataframe(crawled_data_df)
# 
# with tab3:
#     st.write("### üì• Export Report")
#     st.download_button(
#         "Download CSV",
#         data=crawled_data_df.to_csv(index=False).encode("utf-8"),
#         file_name="report.csv",
#         mime="text/csv"
#     )
#

!streamlit run app.py &

from pyngrok import ngrok

ngrok.kill()  # Important: Avoid multiple tunnel issues

public_url = ngrok.connect(8501)
print("üåê Public URL:", public_url)

import streamlit as st
import pandas as pd

st.set_page_config(page_title="IR Project - Member 4", layout="wide")

st.title("üöÄ Intelligent Web Crawler (Member 4 UI)")
tab1, tab2, tab3 = st.tabs(["Overview", "Analysis Results", "Report"])

with tab1:
    st.markdown("### üìå Project Overview")
    st.markdown("This is the Member 4 UI for displaying the crawler output.")

# Sample fake data for testing
crawled_data_df = pd.DataFrame({
    "URL": ["https://example.com", "https://openai.com"],
    "Title": ["Example Domain", "OpenAI"],
    "Keywords": [["example", "domain"], ["AI", "machine learning"]],
})

with tab2:
    st.markdown("### üîç Analysis Results")
    st.dataframe(crawled_data_df)

with tab3:
    st.markdown("### üì§ Download Report")
    st.download_button("Download CSV", crawled_data_df.to_csv(index=False), "results.csv", "text/csv")

    !pip install pyngrok streamlit -q

from pyngrok import ngrok
import threading

# Save the streamlit script
with open("app.py", "w") as f:
    f.write(open("/content/app.py").read())  # Replace this with actual Streamlit script or use write() directly

# Start Streamlit in a background thread
def start_streamlit():
    !streamlit run app.py

thread = threading.Thread(target=start_streamlit)
thread.start()

# Create public URL
public_url = ngrok.connect(8501)
print("üåê Public URL:", public_url)

import subprocess
subprocess.Popen(["streamlit", "run", "app.py"])

!cat app.py

!nohup streamlit run app.py &
!npx ngrok http 8501

# Install ngrok only once
!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip -o ngrok-stable-linux-amd64.zip
!mv ngrok /usr/local/bin

# Replace with your actual authtoken from https://dashboard.ngrok.com/get-started/your-authtoken
!ngrok config add-authtoken 2xNJZIxkc99D6Mis5rWHwGD9CPG_62Gf5EbNBfTaUiMHSiYgQ

!nohup streamlit run app.py &
!ngrok http 8501

# Commented out IPython magic to ensure Python compatibility.
# %%writefile crawler.py
# import pandas as pd
# 
# def run_crawler():
#     data = {
#         "URL": [
#             "https://example.com",
#             "https://openai.com",
#             "https://streamlit.io"
#         ],
#         "Title": [
#             "Example Domain",
#             "OpenAI ‚Äì Discovering the Future of AI",
#             "Streamlit ‚Äî The fastest way to build data apps"
#         ],
#         "Status": [
#             "200 OK",
#             "200 OK",
#             "200 OK"
#         ]
#     }
#     return pd.DataFrame(data)
#

!pip install -q streamlit ngrok

# Use your ngrok authtoken if needed
# !ngrok config add-authtoken YOUR_TOKEN

# Run Streamlit with ngrok tunnel
!nohup streamlit run app.py & npx ngrok http 8501

!yes | npx ngrok@5.0.0-beta.2 http 8501

!pip install streamlit pyngrok

from pyngrok import ngrok

ngrok.set_auth_token("2xNJZIxkc99D6Mis5rWHwGD9CPG_62Gf5EbNBfTaUiMHSiYgQ")  # Replace with your token

!nohup streamlit run app.py &

from pyngrok import ngrok

# Connect the local Streamlit server to the internet
public_url = ngrok.connect(8501)
print("üåê Streamlit app is live at:", public_url)

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

def is_valid_url(url):
    parsed = urlparse(url)
    return bool(parsed.netloc) and bool(parsed.scheme)

def crawl_website(start_url, max_links=20):
    visited = set()
    to_visit = [start_url]
    results = []

    while to_visit and len(visited) < max_links:
        url = to_visit.pop(0)
        if url in visited:
            continue
        try:
            response = requests.get(url, timeout=5)
            visited.add(url)
            soup = BeautifulSoup(response.text, "html.parser")
            results.append({
                "url": url,
                "title": soup.title.string if soup.title else "No Title",
                "num_links": len(soup.find_all("a"))
            })
            for link_tag in soup.find_all("a", href=True):
                link = urljoin(url, link_tag['href'])
                if is_valid_url(link) and link not in visited:
                    to_visit.append(link)
        except Exception as e:
            results.append({"url": url, "error": str(e)})

    return results

# Commented out IPython magic to ensure Python compatibility.
# %%writefile crawler.py
# import requests
# from bs4 import BeautifulSoup
# from urllib.parse import urljoin, urlparse
# 
# def is_valid_url(url):
#     parsed = urlparse(url)
#     return bool(parsed.netloc) and bool(parsed.scheme)
# 
# def crawl_website(start_url, max_links=20):
#     visited = set()
#     to_visit = [start_url]
#     results = []
# 
#     while to_visit and len(visited) < max_links:
#         url = to_visit.pop(0)
#         if url in visited:
#             continue
#         try:
#             response = requests.get(url, timeout=5)
#             visited.add(url)
#             soup = BeautifulSoup(response.text, "html.parser")
#             results.append({
#                 "url": url,
#                 "title": soup.title.string if soup.title else "No Title",
#                 "num_links": len(soup.find_all("a"))
#             })
#             for link_tag in soup.find_all("a", href=True):
#                 link = urljoin(url, link_tag['href'])
#                 if is_valid_url(link) and link not in visited:
#                     to_visit.append(link)
#         except Exception as e:
#             results.append({"url": url, "error": str(e)})
# 
#     return results
#

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from crawler import crawl_website
# 
# st.set_page_config(page_title="IR Web Crawler", layout="wide")
# 
# tab1, tab2, tab3 = st.tabs(["üìå Project Overview", "üîç Analysis Results", "üì§ Download Report"])
# 
# with tab1:
#     st.title("IR Project: Intelligent Web Crawler & Analyzer")
#     st.write("üë§ omar ehab 232125 - UI & Report Designer")
#     st.markdown("### Description")
#     st.write("This application allows you to run a web crawler, analyze results, and download the report.")
# 
# with tab2:
#     st.subheader("Analysis Results")
#     start_url = st.text_input("Enter a URL to crawl", value="https://example.com")
#     max_links = st.slider("Max pages to crawl", 5, 50, 10)
# 
#     if st.button("Start Crawling"):
#         with st.spinner("Crawling in progress..."):
#             results = crawl_website(start_url, max_links=max_links)
#         st.success(f"‚úÖ Crawled {len(results)} pages.")
#         for res in results:
#             if "error" in res:
#                 st.error(f"{res['url']} - ‚ùå {res['error']}")
#             else:
#                 st.write(f"üîó [{res['title']}]({res['url']}) - {res['num_links']} links found")
# 
# with tab3:
#     st.subheader("Download Final Report")
#     st.write("üìÑ This section will be implemented soon...")
#

!nohup streamlit run app.py &

from pyngrok import ngrok
public_url = ngrok.connect(8501)
print(f"üåê App URL: {public_url}")

pip install streamlit beautifulsoup4 requests fpdf pandas altair